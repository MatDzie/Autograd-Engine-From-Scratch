{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from draw.ipynb\n",
      "importing Jupyter notebook from engine.ipynb\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from draw import draw_dot\n",
    "from engine import Value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron visual representation in Convolutional Neural Network\n",
    "![Neuron](neuron.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple neuron code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label = 'x1')\n",
    "x2 = Value(0.0, label = 'x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label = 'w1')\n",
    "w2 = Value(1.0, label = 'w2')\n",
    "# bias b (value for convenient manual gradient calculation)\n",
    "b = Value(6.8813735870195432, label = 'b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1 * w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2 * w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
    "activation = x1w1x2w2 + b; activation.label = 'activation'\n",
    "output = activation.tanh(); output.label = 'output'\n",
    "\n",
    "# automatic backprogation\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1644pt\" height=\"210pt\"\n",
       " viewBox=\"0.00 0.00 1643.75 210.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 206)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-206 1639.75,-206 1639.75,4 -4,4\"/>\n",
       "<!-- 1502600650256 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1502600650256</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"678.75,-82.5 678.75,-118.5 937.5,-118.5 937.5,-82.5 678.75,-82.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"724.62\" y=\"-95.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x1w1 + x2w2</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"770.5,-82.75 770.5,-118.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"813\" y=\"-95.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"855.5,-82.75 855.5,-118.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"896.5\" y=\"-95.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n",
       "</g>\n",
       "<!-- 1502600649968+ -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>1502600649968+</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1000.5\" cy=\"-127.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1000.5\" y=\"-122.08\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 1502600650256&#45;&gt;1502600649968+ -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>1502600650256&#45;&gt;1502600649968+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M935.84,-118.47C945.43,-119.83 954.53,-121.12 962.7,-122.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"962.18,-125.88 972.57,-123.82 963.16,-118.95 962.18,-125.88\"/>\n",
       "</g>\n",
       "<!-- 1502600650256+ -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1502600650256+</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"615.75\" cy=\"-100.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"615.75\" y=\"-95.08\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 1502600650256+&#45;&gt;1502600650256 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1502600650256+&#45;&gt;1502600650256</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M643.22,-100.5C650.49,-100.5 658.91,-100.5 668.01,-100.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"667.98,-104 677.98,-100.5 667.98,-97 667.98,-104\"/>\n",
       "</g>\n",
       "<!-- 1502600646176 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1502600646176</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1421.25,-109.5 1421.25,-145.5 1635.75,-145.5 1635.75,-109.5 1421.25,-109.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1447.25\" y=\"-122.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1473.25,-109.75 1473.25,-145.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1513.5\" y=\"-122.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.7071</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1553.75,-109.75 1553.75,-145.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1594.75\" y=\"-122.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 1.0000</text>\n",
       "</g>\n",
       "<!-- 1502600646176tanh -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1502600646176tanh</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1358.25\" cy=\"-127.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1358.25\" y=\"-122.08\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">tanh</text>\n",
       "</g>\n",
       "<!-- 1502600646176tanh&#45;&gt;1502600646176 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1502600646176tanh&#45;&gt;1502600646176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1385.54,-127.5C1392.91,-127.5 1401.44,-127.5 1410.57,-127.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1410.54,-131 1420.54,-127.5 1410.54,-124 1410.54,-131\"/>\n",
       "</g>\n",
       "<!-- 1502337806880 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>1502337806880</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1.5,-55.5 1.5,-91.5 198,-91.5 198,-55.5 1.5,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"16.25\" y=\"-68.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x1</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"31,-55.75 31,-91.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"71.25\" y=\"-68.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 2.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"111.5,-55.75 111.5,-91.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.75\" y=\"-68.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad &#45;1.5000</text>\n",
       "</g>\n",
       "<!-- 1502600652176* -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>1502600652176*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"262.5\" cy=\"-73.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.5\" y=\"-68.08\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 1502337806880&#45;&gt;1502600652176* -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>1502337806880&#45;&gt;1502600652176*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M197.97,-73.5C207.23,-73.5 216.17,-73.5 224.28,-73.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"224.14,-77 234.14,-73.5 224.14,-70 224.14,-77\"/>\n",
       "</g>\n",
       "<!-- 1502337809040 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1502337809040</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"3.75,-165.5 3.75,-201.5 195.75,-201.5 195.75,-165.5 3.75,-165.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"18.5\" y=\"-178.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x2</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"33.25,-165.75 33.25,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"73.5\" y=\"-178.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"113.75,-165.75 113.75,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.75\" y=\"-178.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n",
       "</g>\n",
       "<!-- 1502600650544* -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>1502600650544*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"262.5\" cy=\"-128.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.5\" y=\"-123.08\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 1502337809040&#45;&gt;1502600650544* -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>1502337809040&#45;&gt;1502600650544*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171.84,-165.51C181.21,-162.72 190.64,-159.69 199.5,-156.5 209.81,-152.79 220.79,-148.09 230.6,-143.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"231.74,-146.47 239.32,-139.06 228.78,-140.13 231.74,-146.47\"/>\n",
       "</g>\n",
       "<!-- 1502337807024 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>1502337807024</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"715.12,-137.5 715.12,-173.5 901.12,-173.5 901.12,-137.5 715.12,-137.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"726.88\" y=\"-150.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"738.62,-137.75 738.62,-173.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"778.88\" y=\"-150.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 6.8814</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"819.12,-137.75 819.12,-173.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"860.12\" y=\"-150.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n",
       "</g>\n",
       "<!-- 1502337807024&#45;&gt;1502600649968+ -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1502337807024&#45;&gt;1502600649968+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M900.97,-141.99C923.07,-138.74 945.41,-135.46 963.22,-132.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"963.56,-136.18 972.95,-131.26 962.54,-129.25 963.56,-136.18\"/>\n",
       "</g>\n",
       "<!-- 1502600649968 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>1502600649968</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1063.5,-109.5 1063.5,-145.5 1295.25,-145.5 1295.25,-109.5 1063.5,-109.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1098.12\" y=\"-122.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">activation</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1132.75,-109.75 1132.75,-145.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1173\" y=\"-122.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.8814</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1213.25,-109.75 1213.25,-145.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1254.25\" y=\"-122.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n",
       "</g>\n",
       "<!-- 1502600649968&#45;&gt;1502600646176tanh -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>1502600649968&#45;&gt;1502600646176tanh</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1294.89,-127.5C1303.71,-127.5 1312.15,-127.5 1319.82,-127.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1319.8,-131 1329.8,-127.5 1319.8,-124 1319.8,-131\"/>\n",
       "</g>\n",
       "<!-- 1502600649968+&#45;&gt;1502600649968 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1502600649968+&#45;&gt;1502600649968</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1027.98,-127.5C1035.25,-127.5 1043.65,-127.5 1052.68,-127.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1052.54,-131 1062.54,-127.5 1052.54,-124 1052.54,-131\"/>\n",
       "</g>\n",
       "<!-- 1502337809664 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>1502337809664</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-36.5 199.5,-36.5 199.5,-0.5 0,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"16.25\" y=\"-13.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">w1</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"32.5,-0.75 32.5,-36.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"75\" y=\"-13.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;3.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"117.5,-0.75 117.5,-36.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"158.5\" y=\"-13.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 1.0000</text>\n",
       "</g>\n",
       "<!-- 1502337809664&#45;&gt;1502600652176* -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>1502337809664&#45;&gt;1502600652176*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M168.6,-36.43C179.02,-39.56 189.6,-42.96 199.5,-46.5 209.57,-50.1 220.32,-54.56 229.99,-58.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"228.42,-62.38 238.98,-63.28 231.29,-56 228.42,-62.38\"/>\n",
       "</g>\n",
       "<!-- 1502600650544 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>1502600650544</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"327.75,-110.5 327.75,-146.5 550.5,-146.5 550.5,-110.5 327.75,-110.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"357.88\" y=\"-123.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x2 * w2</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"388,-110.75 388,-146.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"428.25\" y=\"-123.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"468.5,-110.75 468.5,-146.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"509.5\" y=\"-123.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n",
       "</g>\n",
       "<!-- 1502600650544&#45;&gt;1502600650256+ -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>1502600650544&#45;&gt;1502600650256+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M550.22,-110.85C560.26,-109.24 569.87,-107.7 578.46,-106.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"578.8,-109.65 588.12,-104.61 577.69,-102.74 578.8,-109.65\"/>\n",
       "</g>\n",
       "<!-- 1502600650544*&#45;&gt;1502600650544 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1502600650544*&#45;&gt;1502600650544</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M289.64,-128.5C297.69,-128.5 307.14,-128.5 317.3,-128.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"317.11,-132 327.11,-128.5 317.11,-125 317.11,-132\"/>\n",
       "</g>\n",
       "<!-- 1502337807696 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>1502337807696</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2.25,-110.5 2.25,-146.5 197.25,-146.5 197.25,-110.5 2.25,-110.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"18.5\" y=\"-123.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">w2</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"34.75,-110.75 34.75,-146.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"75\" y=\"-123.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 1.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"115.25,-110.75 115.25,-146.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"156.25\" y=\"-123.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.0000</text>\n",
       "</g>\n",
       "<!-- 1502337807696&#45;&gt;1502600650544* -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>1502337807696&#45;&gt;1502600650544*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M197.04,-128.5C206.6,-128.5 215.85,-128.5 224.22,-128.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"224.02,-132 234.02,-128.5 224.02,-125 224.02,-132\"/>\n",
       "</g>\n",
       "<!-- 1502600652176 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>1502600652176</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"325.5,-55.5 325.5,-91.5 552.75,-91.5 552.75,-55.5 325.5,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"355.62\" y=\"-68.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x1 * w1</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"385.75,-55.75 385.75,-91.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"428.25\" y=\"-68.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"470.75,-55.75 470.75,-91.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"511.75\" y=\"-68.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n",
       "</g>\n",
       "<!-- 1502600652176&#45;&gt;1502600650256+ -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>1502600652176&#45;&gt;1502600650256+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M552.69,-90.9C561.74,-92.3 570.39,-93.64 578.21,-94.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"577.6,-98.45 588.01,-96.52 578.67,-91.53 577.6,-98.45\"/>\n",
       "</g>\n",
       "<!-- 1502600652176*&#45;&gt;1502600652176 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1502600652176*&#45;&gt;1502600652176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M289.64,-73.5C297.11,-73.5 305.77,-73.5 315.1,-73.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"314.87,-77 324.87,-73.5 314.87,-70 314.87,-77\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x15dca4faf20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_dot(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network visual representation with corresponding code example\n",
    "![Neural_network](neural_network.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, numberOfInputs):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(numberOfInputs)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "        \n",
    "    # forward pass for single neuron\n",
    "    def __call__(self, x):\n",
    "        # activation = sum(wi * xi) + b\n",
    "        activation = sum(wi * xi for wi, xi in zip(self.w, x)) + self.b\n",
    "        output = activation.tanh()\n",
    "        return output\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.9826579760151293)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single neuron instance with random weights and bias\n",
    "x = [2.0, 3.0]\n",
    "neuron = Neuron(len(x))\n",
    "neuron(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer of neurons is just a list of neurons equal to numberOfOutputs, where numberOfInputs means how many inputs each neuron in layer has\n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self, numberOfInputs, numberOfOutputs):\n",
    "        self.neurons = [Neuron(numberOfInputs) for _ in range(numberOfOutputs)]\n",
    "        \n",
    "    # forward pass for layer, takes list of inputs and returns list of outputs\n",
    "    def __call__(self, inputs):\n",
    "        outputs = [neuron(inputs) for neuron in self.neurons]\n",
    "        return outputs[0] if len(outputs) == 1 else outputs\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            params += neuron.parameters()\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9993077157521024),\n",
       " Value(data=0.9991226175849671),\n",
       " Value(data=0.9998926613271418),\n",
       " Value(data=0.9999809078171394)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single layer of 4 neurons where each neuron has 3 inputs equal to x\n",
    "x = [2.0, 3.0, 4.0]\n",
    "layer = Layer(len(x), 4)\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    \n",
    "    def __init__(self, numberOfInputs, layerSizes):\n",
    "        # create layerSizes list from value inputLayerSize and concatenate it with hiddenLayerSizes\n",
    "        layerSizes = [numberOfInputs] + layerSizes\n",
    "        self.layers = [Layer(layerSizes[i], layerSizes[i+1]) for i in range(len(layerSizes)-1)]\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # call each layer with inputs and set outputs as new inputs for next layer\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(inputs)\n",
    "            inputs = outputs\n",
    "        return outputs\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters()\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.9023609044599542)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [2.0, 3.0, 4.0]\n",
    "mlp = MultiLayerPerceptron(len(inputs), [4, 4, 1])\n",
    "mlp(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataSet = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "desiredOutputs = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss - single number that measures how well neural netowrk is performing, lower is better.\n",
    "\n",
    "### Loss = sum((output of neural network - desired output)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.22619936115676745),\n",
       " Value(data=-0.7577463579399697),\n",
       " Value(data=-0.7797296273110049),\n",
       " Value(data=-0.28591211207018175)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate outputs for each input in inputDataSet before training\n",
    "predictedOutputs = [mlp(inputs) for inputs in inputDataSet]\n",
    "predictedOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.3595432528188884)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate loss\n",
    "loss = sum((predicted - desired)**2 for desired, predicted in zip(desiredOutputs, predictedOutputs))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters - this is small neural network with only 41 parameters which include all weights and biases\n",
    "len(mlp.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training by doing gradient descent. \n",
    "Our goal is to adjust weights and biases to minimize loss function.\n",
    "\n",
    "forward pass -> calculate loss -> backpropagation -> adjust weights and biases -> repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 2.3595432528188884\n",
      "Iteration: 1, Loss: 1.3282424301733111\n",
      "Iteration: 2, Loss: 0.8974688124184611\n",
      "Iteration: 3, Loss: 0.7049644302508686\n",
      "Iteration: 4, Loss: 0.5980238592407905\n",
      "Iteration: 5, Loss: 0.5271309532143471\n",
      "Iteration: 6, Loss: 0.47439331340933233\n",
      "Iteration: 7, Loss: 0.43231638098563674\n",
      "Iteration: 8, Loss: 0.3973013330863205\n",
      "Iteration: 9, Loss: 0.3673933207199256\n",
      "Iteration: 10, Loss: 0.3414090620334206\n",
      "Iteration: 11, Loss: 0.3185648508463267\n",
      "Iteration: 12, Loss: 0.2983037902357084\n",
      "Iteration: 13, Loss: 0.2802088680239651\n",
      "Iteration: 14, Loss: 0.26395573571631015\n",
      "Iteration: 15, Loss: 0.24928511699512454\n",
      "Iteration: 16, Loss: 0.23598556890535888\n",
      "Iteration: 17, Loss: 0.2238820577387201\n",
      "Iteration: 18, Loss: 0.21282800363052684\n",
      "Iteration: 19, Loss: 0.20269951326642272\n",
      "Iteration: 20, Loss: 0.19339106340288997\n",
      "Iteration: 21, Loss: 0.1848121884384126\n",
      "Iteration: 22, Loss: 0.17688488810819836\n",
      "Iteration: 23, Loss: 0.16954156694087485\n",
      "Iteration: 24, Loss: 0.16272337573625084\n",
      "Iteration: 25, Loss: 0.15637886279934798\n",
      "Iteration: 26, Loss: 0.15046286754174265\n",
      "Iteration: 27, Loss: 0.14493560613174358\n",
      "Iteration: 28, Loss: 0.13976191093132082\n",
      "Iteration: 29, Loss: 0.1349105941841387\n",
      "Iteration: 30, Loss: 0.1303539128672171\n",
      "Iteration: 31, Loss: 0.12606711646689095\n",
      "Iteration: 32, Loss: 0.12202806313859416\n",
      "Iteration: 33, Loss: 0.11821689256707962\n",
      "Iteration: 34, Loss: 0.1146157460740379\n",
      "Iteration: 35, Loss: 0.11120852627713085\n",
      "Iteration: 36, Loss: 0.10798068999971117\n",
      "Iteration: 37, Loss: 0.10491906924633261\n",
      "Iteration: 38, Loss: 0.10201171595715852\n",
      "Iteration: 39, Loss: 0.09924776698124113\n",
      "Iteration: 40, Loss: 0.09661732630006296\n",
      "Iteration: 41, Loss: 0.09411136201627943\n",
      "Iteration: 42, Loss: 0.09172161601973722\n",
      "Iteration: 43, Loss: 0.08944052457038199\n",
      "Iteration: 44, Loss: 0.08726114830887559\n",
      "Iteration: 45, Loss: 0.08517711043116513\n",
      "Iteration: 46, Loss: 0.08318254195126203\n",
      "Iteration: 47, Loss: 0.08127203313385534\n",
      "Iteration: 48, Loss: 0.07944059031051645\n",
      "Iteration: 49, Loss: 0.07768359740455252\n",
      "Iteration: 50, Loss: 0.07599678158358134\n",
      "Iteration: 51, Loss: 0.07437618253857146\n",
      "Iteration: 52, Loss: 0.07281812495576723\n",
      "Iteration: 53, Loss: 0.07131919380557586\n",
      "Iteration: 54, Loss: 0.06987621212173677\n",
      "Iteration: 55, Loss: 0.0684862209862604\n",
      "Iteration: 56, Loss: 0.06714646147180858\n",
      "Iteration: 57, Loss: 0.06585435832432554\n",
      "Iteration: 58, Loss: 0.06460750519557673\n",
      "Iteration: 59, Loss: 0.06340365125845059\n",
      "Iteration: 60, Loss: 0.06224068905797342\n",
      "Iteration: 61, Loss: 0.06111664346842387\n",
      "Iteration: 62, Loss: 0.06002966164209818\n",
      "Iteration: 63, Loss: 0.05897800384849024\n",
      "Iteration: 64, Loss: 0.05796003511418267\n",
      "Iteration: 65, Loss: 0.05697421758383702\n",
      "Iteration: 66, Loss: 0.05601910353150789\n",
      "Iteration: 67, Loss: 0.05509332895926583\n",
      "Iteration: 68, Loss: 0.05419560772693418\n",
      "Iteration: 69, Loss: 0.05332472616275818\n",
      "Iteration: 70, Loss: 0.05247953811011866\n",
      "Iteration: 71, Loss: 0.05165896037009277\n",
      "Iteration: 72, Loss: 0.050861968503804526\n",
      "Iteration: 73, Loss: 0.05008759296218401\n",
      "Iteration: 74, Loss: 0.049334915514015185\n",
      "Iteration: 75, Loss: 0.04860306594604878\n",
      "Iteration: 76, Loss: 0.047891219011539546\n",
      "Iteration: 77, Loss: 0.04719859160586679\n",
      "Iteration: 78, Loss: 0.046524440149948436\n",
      "Iteration: 79, Loss: 0.045868058163996875\n",
      "Iteration: 80, Loss: 0.04522877401580601\n",
      "Iteration: 81, Loss: 0.0446059488292279\n",
      "Iteration: 82, Loss: 0.04399897453982368\n",
      "Iteration: 83, Loss: 0.043407272085853425\n",
      "Iteration: 84, Loss: 0.04283028972384059\n",
      "Iteration: 85, Loss: 0.04226750145890439\n",
      "Iteration: 86, Loss: 0.04171840558092031\n",
      "Iteration: 87, Loss: 0.041182523298350614\n",
      "Iteration: 88, Loss: 0.04065939746229176\n",
      "Iteration: 89, Loss: 0.04014859137392299\n",
      "Iteration: 90, Loss: 0.03964968766912043\n",
      "Iteration: 91, Loss: 0.039162287274522554\n",
      "Iteration: 92, Loss: 0.038686008429808606\n",
      "Iteration: 93, Loss: 0.038220485771383\n",
      "Iteration: 94, Loss: 0.03776536947305002\n",
      "Iteration: 95, Loss: 0.03732032443962124\n",
      "Iteration: 96, Loss: 0.0368850295497203\n",
      "Iteration: 97, Loss: 0.03645917694434843\n",
      "Iteration: 98, Loss: 0.03604247135804182\n",
      "Iteration: 99, Loss: 0.03563462948969974\n",
      "Iteration: 100, Loss: 0.03523537941038704\n",
      "Iteration: 101, Loss: 0.03484446000562105\n",
      "Iteration: 102, Loss: 0.03446162044984101\n",
      "Iteration: 103, Loss: 0.0340866197109309\n",
      "Iteration: 104, Loss: 0.033719226082825834\n",
      "Iteration: 105, Loss: 0.033359216744375975\n",
      "Iteration: 106, Loss: 0.0330063773427771\n",
      "Iteration: 107, Loss: 0.032660501599998046\n",
      "Iteration: 108, Loss: 0.032321390940748575\n",
      "Iteration: 109, Loss: 0.03198885414063604\n",
      "Iteration: 110, Loss: 0.03166270699325293\n",
      "Iteration: 111, Loss: 0.031342771995026476\n",
      "Iteration: 112, Loss: 0.031028878046743126\n",
      "Iteration: 113, Loss: 0.030720860170735623\n",
      "Iteration: 114, Loss: 0.030418559242787427\n",
      "Iteration: 115, Loss: 0.03012182173787748\n",
      "Iteration: 116, Loss: 0.029830499488942938\n",
      "Iteration: 117, Loss: 0.029544449457896374\n",
      "Iteration: 118, Loss: 0.029263533518180562\n",
      "Iteration: 119, Loss: 0.02898761824819472\n",
      "Iteration: 120, Loss: 0.028716574734967144\n",
      "Iteration: 121, Loss: 0.028450278387490615\n",
      "Iteration: 122, Loss: 0.028188608759173648\n",
      "Iteration: 123, Loss: 0.027931449378897414\n",
      "Iteration: 124, Loss: 0.027678687590196708\n",
      "Iteration: 125, Loss: 0.027430214398118956\n",
      "Iteration: 126, Loss: 0.027185924323337125\n",
      "Iteration: 127, Loss: 0.026945715263123365\n",
      "Iteration: 128, Loss: 0.02670948835881203\n",
      "Iteration: 129, Loss: 0.026477147869403524\n",
      "Iteration: 130, Loss: 0.02624860105098189\n",
      "Iteration: 131, Loss: 0.026023758041639387\n",
      "Iteration: 132, Loss: 0.025802531751617664\n",
      "Iteration: 133, Loss: 0.025584837758394458\n",
      "Iteration: 134, Loss: 0.025370594206459502\n",
      "Iteration: 135, Loss: 0.02515972171153839\n",
      "Iteration: 136, Loss: 0.024952143269037413\n",
      "Iteration: 137, Loss: 0.024747784166495423\n",
      "Iteration: 138, Loss: 0.024546571899840934\n",
      "Iteration: 139, Loss: 0.024348436093264134\n",
      "Iteration: 140, Loss: 0.024153308422524333\n",
      "Iteration: 141, Loss: 0.023961122541522696\n",
      "Iteration: 142, Loss: 0.02377181401198181\n",
      "Iteration: 143, Loss: 0.023585320236078663\n",
      "Iteration: 144, Loss: 0.023401580391890244\n",
      "Iteration: 145, Loss: 0.023220535371515263\n",
      "Iteration: 146, Loss: 0.02304212772174465\n",
      "Iteration: 147, Loss: 0.022866301587160444\n",
      "Iteration: 148, Loss: 0.022693002655547773\n",
      "Iteration: 149, Loss: 0.022522178105512485\n",
      "Iteration: 150, Loss: 0.022353776556201067\n",
      "Iteration: 151, Loss: 0.022187748019026007\n",
      "Iteration: 152, Loss: 0.0220240438513051\n",
      "Iteration: 153, Loss: 0.021862616711725635\n",
      "Iteration: 154, Loss: 0.021703420517553126\n",
      "Iteration: 155, Loss: 0.02154641040350306\n",
      "Iteration: 156, Loss: 0.021391542682203894\n",
      "Iteration: 157, Loss: 0.021238774806178466\n",
      "Iteration: 158, Loss: 0.021088065331277467\n",
      "Iteration: 159, Loss: 0.020939373881501085\n",
      "Iteration: 160, Loss: 0.020792661115147534\n",
      "Iteration: 161, Loss: 0.020647888692231678\n",
      "Iteration: 162, Loss: 0.02050501924311797\n",
      "Iteration: 163, Loss: 0.02036401633831615\n",
      "Iteration: 164, Loss: 0.020224844459389567\n",
      "Iteration: 165, Loss: 0.02008746897092936\n",
      "Iteration: 166, Loss: 0.01995185609354893\n",
      "Iteration: 167, Loss: 0.019817972877856655\n",
      "Iteration: 168, Loss: 0.01968578717936477\n",
      "Iteration: 169, Loss: 0.019555267634297338\n",
      "Iteration: 170, Loss: 0.019426383636258232\n",
      "Iteration: 171, Loss: 0.019299105313725428\n",
      "Iteration: 172, Loss: 0.019173403508337072\n",
      "Iteration: 173, Loss: 0.01904924975393723\n",
      "Iteration: 174, Loss: 0.018926616256351253\n",
      "Iteration: 175, Loss: 0.01880547587386044\n",
      "Iteration: 176, Loss: 0.01868580209834933\n",
      "Iteration: 177, Loss: 0.01856756903709765\n",
      "Iteration: 178, Loss: 0.018450751395192873\n",
      "Iteration: 179, Loss: 0.018335324458537243\n",
      "Iteration: 180, Loss: 0.01822126407742819\n",
      "Iteration: 181, Loss: 0.018108546650687694\n",
      "Iteration: 182, Loss: 0.017997149110321076\n",
      "Iteration: 183, Loss: 0.017887048906684008\n",
      "Iteration: 184, Loss: 0.01777822399413847\n",
      "Iteration: 185, Loss: 0.017670652817179056\n",
      "Iteration: 186, Loss: 0.017564314297011996\n",
      "Iteration: 187, Loss: 0.01745918781856956\n",
      "Iteration: 188, Loss: 0.01735525321794399\n",
      "Iteration: 189, Loss: 0.01725249077022457\n",
      "Iteration: 190, Loss: 0.017150881177723856\n",
      "Iteration: 191, Loss: 0.01705040555857796\n",
      "Iteration: 192, Loss: 0.016951045435707433\n",
      "Iteration: 193, Loss: 0.01685278272612561\n",
      "Iteration: 194, Loss: 0.01675559973058174\n",
      "Iteration: 195, Loss: 0.01665947912352686\n",
      "Iteration: 196, Loss: 0.016564403943390656\n",
      "Iteration: 197, Loss: 0.016470357583158467\n",
      "Iteration: 198, Loss: 0.016377323781237345\n",
      "Iteration: 199, Loss: 0.01628528661260114\n",
      "Iteration: 200, Loss: 0.016194230480205313\n",
      "Iteration: 201, Loss: 0.016104140106660426\n",
      "Iteration: 202, Loss: 0.01601500052615729\n",
      "Iteration: 203, Loss: 0.015926797076633636\n",
      "Iteration: 204, Loss: 0.015839515392174243\n",
      "Iteration: 205, Loss: 0.015753141395636938\n",
      "Iteration: 206, Loss: 0.015667661291495986\n",
      "Iteration: 207, Loss: 0.015583061558896407\n",
      "Iteration: 208, Loss: 0.015499328944910797\n",
      "Iteration: 209, Loss: 0.015416450457993263\n",
      "Iteration: 210, Loss: 0.015334413361622631\n",
      "Iteration: 211, Loss: 0.01525320516812914\n",
      "Iteration: 212, Loss: 0.0151728136326986\n",
      "Iteration: 213, Loss: 0.015093226747547763\n",
      "Iteration: 214, Loss: 0.015014432736265584\n",
      "Iteration: 215, Loss: 0.01493642004831476\n",
      "Iteration: 216, Loss: 0.014859177353688222\n",
      "Iteration: 217, Loss: 0.014782693537715667\n",
      "Iteration: 218, Loss: 0.014706957696015481\n",
      "Iteration: 219, Loss: 0.01463195912958664\n",
      "Iteration: 220, Loss: 0.01455768734003704\n",
      "Iteration: 221, Loss: 0.014484132024943006\n",
      "Iteration: 222, Loss: 0.014411283073336464\n",
      "Iteration: 223, Loss: 0.01433913056131542\n",
      "Iteration: 224, Loss: 0.014267664747773818\n",
      "Iteration: 225, Loss: 0.01419687607024708\n",
      "Iteration: 226, Loss: 0.014126755140869725\n",
      "Iteration: 227, Loss: 0.01405729274244209\n",
      "Iteration: 228, Loss: 0.013988479824600978\n",
      "Iteration: 229, Loss: 0.013920307500093812\n",
      "Iteration: 230, Loss: 0.013852767041150687\n",
      "Iteration: 231, Loss: 0.013785849875952171\n",
      "Iteration: 232, Loss: 0.013719547585190537\n",
      "Iteration: 233, Loss: 0.01365385189872053\n",
      "Iteration: 234, Loss: 0.013588754692297719\n",
      "Iteration: 235, Loss: 0.013524247984401548\n",
      "Iteration: 236, Loss: 0.01346032393314029\n",
      "Iteration: 237, Loss: 0.013396974833235938\n",
      "Iteration: 238, Loss: 0.013334193113086396\n",
      "Iteration: 239, Loss: 0.01327197133190259\n",
      "Iteration: 240, Loss: 0.013210302176918435\n",
      "Iteration: 241, Loss: 0.013149178460671453\n",
      "Iteration: 242, Loss: 0.0130885931183521\n",
      "Iteration: 243, Loss: 0.013028539205219593\n",
      "Iteration: 244, Loss: 0.012969009894082336\n",
      "Iteration: 245, Loss: 0.012909998472841262\n",
      "Iteration: 246, Loss: 0.012851498342094318\n",
      "Iteration: 247, Loss: 0.012793503012799644\n",
      "Iteration: 248, Loss: 0.012736006103997066\n",
      "Iteration: 249, Loss: 0.012679001340584577\n",
      "Iteration: 250, Loss: 0.012622482551150055\n",
      "Iteration: 251, Loss: 0.012566443665855138\n",
      "Iteration: 252, Loss: 0.012510878714370559\n",
      "Iteration: 253, Loss: 0.012455781823861516\n",
      "Iteration: 254, Loss: 0.012401147217021248\n",
      "Iteration: 255, Loss: 0.012346969210151803\n",
      "Iteration: 256, Loss: 0.012293242211290797\n",
      "Iteration: 257, Loss: 0.01223996071838255\n",
      "Iteration: 258, Loss: 0.012187119317492397\n",
      "Iteration: 259, Loss: 0.012134712681063508\n",
      "Iteration: 260, Loss: 0.012082735566214552\n",
      "Iteration: 261, Loss: 0.012031182813076791\n",
      "Iteration: 262, Loss: 0.011980049343170825\n",
      "Iteration: 263, Loss: 0.01192933015782027\n",
      "Iteration: 264, Loss: 0.011879020336602557\n",
      "Iteration: 265, Loss: 0.01182911503583562\n",
      "Iteration: 266, Loss: 0.011779609487098719\n",
      "Iteration: 267, Loss: 0.011730498995788161\n",
      "Iteration: 268, Loss: 0.01168177893970482\n",
      "Iteration: 269, Loss: 0.011633444767674785\n",
      "Iteration: 270, Loss: 0.01158549199820054\n",
      "Iteration: 271, Loss: 0.011537916218142923\n",
      "Iteration: 272, Loss: 0.011490713081432655\n",
      "Iteration: 273, Loss: 0.011443878307810868\n",
      "Iteration: 274, Loss: 0.011397407681597246\n",
      "Iteration: 275, Loss: 0.011351297050486401\n",
      "Iteration: 276, Loss: 0.0113055423243701\n",
      "Iteration: 277, Loss: 0.011260139474185976\n",
      "Iteration: 278, Loss: 0.011215084530791564\n",
      "Iteration: 279, Loss: 0.011170373583862763\n",
      "Iteration: 280, Loss: 0.011126002780816668\n",
      "Iteration: 281, Loss: 0.011081968325757642\n",
      "Iteration: 282, Loss: 0.011038266478446362\n",
      "Iteration: 283, Loss: 0.010994893553291177\n",
      "Iteration: 284, Loss: 0.010951845918361024\n",
      "Iteration: 285, Loss: 0.010909119994419812\n",
      "Iteration: 286, Loss: 0.01086671225398134\n",
      "Iteration: 287, Loss: 0.010824619220384156\n",
      "Iteration: 288, Loss: 0.010782837466886592\n",
      "Iteration: 289, Loss: 0.010741363615780375\n",
      "Iteration: 290, Loss: 0.01070019433752337\n",
      "Iteration: 291, Loss: 0.010659326349890398\n",
      "Iteration: 292, Loss: 0.010618756417141873\n",
      "Iteration: 293, Loss: 0.010578481349209835\n",
      "Iteration: 294, Loss: 0.010538498000900908\n",
      "Iteration: 295, Loss: 0.010498803271115797\n",
      "Iteration: 296, Loss: 0.010459394102084928\n",
      "Iteration: 297, Loss: 0.010420267478619905\n",
      "Iteration: 298, Loss: 0.010381420427380238\n",
      "Iteration: 299, Loss: 0.010342850016155323\n",
      "Iteration: 300, Loss: 0.010304553353160578\n",
      "Iteration: 301, Loss: 0.010266527586348582\n",
      "Iteration: 302, Loss: 0.01022876990273357\n",
      "Iteration: 303, Loss: 0.010191277527729954\n",
      "Iteration: 304, Loss: 0.010154047724504085\n",
      "Iteration: 305, Loss: 0.010117077793338893\n",
      "Iteration: 306, Loss: 0.010080365071011472\n",
      "Iteration: 307, Loss: 0.010043906930182853\n",
      "Iteration: 308, Loss: 0.010007700778800043\n",
      "Iteration: 309, Loss: 0.0099717440595101\n",
      "Iteration: 310, Loss: 0.009936034249085172\n",
      "Iteration: 311, Loss: 0.009900568857859757\n",
      "Iteration: 312, Loss: 0.009865345429178109\n",
      "Iteration: 313, Loss: 0.009830361538853194\n",
      "Iteration: 314, Loss: 0.009795614794635645\n",
      "Iteration: 315, Loss: 0.009761102835693446\n",
      "Iteration: 316, Loss: 0.00972682333210147\n",
      "Iteration: 317, Loss: 0.009692773984341015\n",
      "Iteration: 318, Loss: 0.009658952522808954\n",
      "Iteration: 319, Loss: 0.009625356707336056\n",
      "Iteration: 320, Loss: 0.009591984326715089\n",
      "Iteration: 321, Loss: 0.009558833198237419\n",
      "Iteration: 322, Loss: 0.009525901167238542\n",
      "Iteration: 323, Loss: 0.009493186106652557\n",
      "Iteration: 324, Loss: 0.009460685916574566\n",
      "Iteration: 325, Loss: 0.009428398523831677\n",
      "Iteration: 326, Loss: 0.009396321881561986\n",
      "Iteration: 327, Loss: 0.009364453968801468\n",
      "Iteration: 328, Loss: 0.009332792790078339\n",
      "Iteration: 329, Loss: 0.009301336375015557\n",
      "Iteration: 330, Loss: 0.009270082777940022\n",
      "Iteration: 331, Loss: 0.009239030077499419\n",
      "Iteration: 332, Loss: 0.009208176376286122\n",
      "Iteration: 333, Loss: 0.009177519800467824\n",
      "Iteration: 334, Loss: 0.009147058499425265\n",
      "Iteration: 335, Loss: 0.009116790645396089\n",
      "Iteration: 336, Loss: 0.009086714433125734\n",
      "Iteration: 337, Loss: 0.009056828079524334\n",
      "Iteration: 338, Loss: 0.009027129823330014\n",
      "Iteration: 339, Loss: 0.008997617924778146\n",
      "Iteration: 340, Loss: 0.008968290665276688\n",
      "Iteration: 341, Loss: 0.008939146347087172\n",
      "Iteration: 342, Loss: 0.008910183293011777\n",
      "Iteration: 343, Loss: 0.008881399846085446\n",
      "Iteration: 344, Loss: 0.008852794369274171\n",
      "Iteration: 345, Loss: 0.008824365245177875\n",
      "Iteration: 346, Loss: 0.008796110875739348\n",
      "Iteration: 347, Loss: 0.008768029681957525\n",
      "Iteration: 348, Loss: 0.008740120103606683\n",
      "Iteration: 349, Loss: 0.00871238059895963\n",
      "Iteration: 350, Loss: 0.008684809644516702\n",
      "Iteration: 351, Loss: 0.008657405734738602\n",
      "Iteration: 352, Loss: 0.008630167381784598\n",
      "Iteration: 353, Loss: 0.00860309311525493\n",
      "Iteration: 354, Loss: 0.00857618148193751\n",
      "Iteration: 355, Loss: 0.008549431045559563\n",
      "Iteration: 356, Loss: 0.008522840386542958\n",
      "Iteration: 357, Loss: 0.008496408101764082\n",
      "Iteration: 358, Loss: 0.00847013280431768\n",
      "Iteration: 359, Loss: 0.008444013123284862\n",
      "Iteration: 360, Loss: 0.00841804770350489\n",
      "Iteration: 361, Loss: 0.008392235205351043\n",
      "Iteration: 362, Loss: 0.008366574304510023\n",
      "Iteration: 363, Loss: 0.008341063691765346\n",
      "Iteration: 364, Loss: 0.00831570207278433\n",
      "Iteration: 365, Loss: 0.008290488167908382\n",
      "Iteration: 366, Loss: 0.008265420711947238\n",
      "Iteration: 367, Loss: 0.008240498453976368\n",
      "Iteration: 368, Loss: 0.00821572015713774\n",
      "Iteration: 369, Loss: 0.00819108459844411\n",
      "Iteration: 370, Loss: 0.00816659056858627\n",
      "Iteration: 371, Loss: 0.008142236871743728\n",
      "Iteration: 372, Loss: 0.008118022325398416\n",
      "Iteration: 373, Loss: 0.008093945760151393\n",
      "Iteration: 374, Loss: 0.00807000601954269\n",
      "Iteration: 375, Loss: 0.008046201959874015\n",
      "Iteration: 376, Loss: 0.008022532450034342\n",
      "Iteration: 377, Loss: 0.007998996371328369\n",
      "Iteration: 378, Loss: 0.007975592617307806\n",
      "Iteration: 379, Loss: 0.007952320093605286\n",
      "Iteration: 380, Loss: 0.007929177717771085\n",
      "Iteration: 381, Loss: 0.007906164419112252\n",
      "Iteration: 382, Loss: 0.007883279138534741\n",
      "Iteration: 383, Loss: 0.007860520828387509\n",
      "Iteration: 384, Loss: 0.007837888452309556\n",
      "Iteration: 385, Loss: 0.007815380985079272\n",
      "Iteration: 386, Loss: 0.0077929974124660295\n",
      "Iteration: 387, Loss: 0.007770736731084417\n",
      "Iteration: 388, Loss: 0.007748597948250471\n",
      "Iteration: 389, Loss: 0.007726580081840367\n",
      "Iteration: 390, Loss: 0.007704682160151282\n",
      "Iteration: 391, Loss: 0.0076829032217644744\n",
      "Iteration: 392, Loss: 0.007661242315410455\n",
      "Iteration: 393, Loss: 0.007639698499836227\n",
      "Iteration: 394, Loss: 0.007618270843674729\n",
      "Iteration: 395, Loss: 0.00759695842531625\n",
      "Iteration: 396, Loss: 0.0075757603327816754\n",
      "Iteration: 397, Loss: 0.007554675663597965\n",
      "Iteration: 398, Loss: 0.007533703524675367\n",
      "Iteration: 399, Loss: 0.007512843032186596\n",
      "Iteration: 400, Loss: 0.007492093311447795\n",
      "Iteration: 401, Loss: 0.007471453496801407\n",
      "Iteration: 402, Loss: 0.007450922731500702\n",
      "Iteration: 403, Loss: 0.007430500167596321\n",
      "Iteration: 404, Loss: 0.00741018496582423\n",
      "Iteration: 405, Loss: 0.007389976295495508\n",
      "Iteration: 406, Loss: 0.007369873334387961\n",
      "Iteration: 407, Loss: 0.007349875268639042\n",
      "Iteration: 408, Loss: 0.0073299812926406185\n",
      "Iteration: 409, Loss: 0.007310190608935309\n",
      "Iteration: 410, Loss: 0.00729050242811431\n",
      "Iteration: 411, Loss: 0.007270915968716639\n",
      "Iteration: 412, Loss: 0.007251430457130167\n",
      "Iteration: 413, Loss: 0.007232045127493834\n",
      "Iteration: 414, Loss: 0.007212759221601536\n",
      "Iteration: 415, Loss: 0.007193571988807159\n",
      "Iteration: 416, Loss: 0.007174482685931397\n",
      "Iteration: 417, Loss: 0.007155490577169572\n",
      "Iteration: 418, Loss: 0.0071365949340010644\n",
      "Iteration: 419, Loss: 0.007117795035099877\n",
      "Iteration: 420, Loss: 0.007099090166246719\n",
      "Iteration: 421, Loss: 0.007080479620242096\n",
      "Iteration: 422, Loss: 0.007061962696821019\n",
      "Iteration: 423, Loss: 0.007043538702568547\n",
      "Iteration: 424, Loss: 0.007025206950836926\n",
      "Iteration: 425, Loss: 0.007006966761663666\n",
      "Iteration: 426, Loss: 0.006988817461690892\n",
      "Iteration: 427, Loss: 0.006970758384085926\n",
      "Iteration: 428, Loss: 0.0069527888684628066\n",
      "Iteration: 429, Loss: 0.00693490826080519\n",
      "Iteration: 430, Loss: 0.006917115913390094\n",
      "Iteration: 431, Loss: 0.006899411184713065\n",
      "Iteration: 432, Loss: 0.00688179343941386\n",
      "Iteration: 433, Loss: 0.006864262048203883\n",
      "Iteration: 434, Loss: 0.006846816387794018\n",
      "Iteration: 435, Loss: 0.006829455840823893\n",
      "Iteration: 436, Loss: 0.006812179795791913\n",
      "Iteration: 437, Loss: 0.006794987646986396\n",
      "Iteration: 438, Loss: 0.006777878794417665\n",
      "Iteration: 439, Loss: 0.006760852643750995\n",
      "Iteration: 440, Loss: 0.00674390860624058\n",
      "Iteration: 441, Loss: 0.006727046098664402\n",
      "Iteration: 442, Loss: 0.006710264543260052\n",
      "Iteration: 443, Loss: 0.006693563367661323\n",
      "Iteration: 444, Loss: 0.006676942004835724\n",
      "Iteration: 445, Loss: 0.006660399893022965\n",
      "Iteration: 446, Loss: 0.006643936475674182\n",
      "Iteration: 447, Loss: 0.006627551201391941\n",
      "Iteration: 448, Loss: 0.0066112435238712824\n",
      "Iteration: 449, Loss: 0.006595012901841198\n",
      "Iteration: 450, Loss: 0.006578858799007409\n",
      "Iteration: 451, Loss: 0.00656278068399547\n",
      "Iteration: 452, Loss: 0.006546778030294924\n",
      "Iteration: 453, Loss: 0.0065308503162039965\n",
      "Iteration: 454, Loss: 0.006514997024775324\n",
      "Iteration: 455, Loss: 0.006499217643762086\n",
      "Iteration: 456, Loss: 0.006483511665565161\n",
      "Iteration: 457, Loss: 0.006467878587180777\n",
      "Iteration: 458, Loss: 0.006452317910148933\n",
      "Iteration: 459, Loss: 0.006436829140502619\n",
      "Iteration: 460, Loss: 0.006421411788717477\n",
      "Iteration: 461, Loss: 0.006406065369662399\n",
      "Iteration: 462, Loss: 0.006390789402550558\n",
      "Iteration: 463, Loss: 0.00637558341089128\n",
      "Iteration: 464, Loss: 0.006360446922442335\n",
      "Iteration: 465, Loss: 0.0063453794691630695\n",
      "Iteration: 466, Loss: 0.006330380587168078\n",
      "Iteration: 467, Loss: 0.0063154498166813865\n",
      "Iteration: 468, Loss: 0.00630058670199138\n",
      "Iteration: 469, Loss: 0.006285790791406299\n",
      "Iteration: 470, Loss: 0.006271061637210194\n",
      "Iteration: 471, Loss: 0.006256398795619636\n",
      "Iteration: 472, Loss: 0.006241801826740792\n",
      "Iteration: 473, Loss: 0.0062272702945272256\n",
      "Iteration: 474, Loss: 0.006212803766738187\n",
      "Iteration: 475, Loss: 0.006198401814897413\n",
      "Iteration: 476, Loss: 0.006184064014252491\n",
      "Iteration: 477, Loss: 0.006169789943734714\n",
      "Iteration: 478, Loss: 0.00615557918591949\n",
      "Iteration: 479, Loss: 0.006141431326987221\n",
      "Iteration: 480, Loss: 0.006127345956684778\n",
      "Iteration: 481, Loss: 0.006113322668287293\n",
      "Iteration: 482, Loss: 0.006099361058560594\n",
      "Iteration: 483, Loss: 0.0060854607277240225\n",
      "Iteration: 484, Loss: 0.006071621279413782\n",
      "Iteration: 485, Loss: 0.006057842320646812\n",
      "Iteration: 486, Loss: 0.006044123461784864\n",
      "Iteration: 487, Loss: 0.006030464316499349\n",
      "Iteration: 488, Loss: 0.0060168645017363714\n",
      "Iteration: 489, Loss: 0.006003323637682478\n",
      "Iteration: 490, Loss: 0.005989841347730423\n",
      "Iteration: 491, Loss: 0.005976417258445854\n",
      "Iteration: 492, Loss: 0.005963050999533968\n",
      "Iteration: 493, Loss: 0.005949742203806972\n",
      "Iteration: 494, Loss: 0.005936490507151554\n",
      "Iteration: 495, Loss: 0.005923295548497151\n",
      "Iteration: 496, Loss: 0.005910156969784268\n",
      "Iteration: 497, Loss: 0.005897074415933436\n",
      "Iteration: 498, Loss: 0.00588404753481441\n",
      "Iteration: 499, Loss: 0.005871075977215899\n",
      "Iteration: 500, Loss: 0.005858159396815346\n",
      "Iteration: 501, Loss: 0.005845297450149493\n",
      "Iteration: 502, Loss: 0.005832489796585037\n",
      "Iteration: 503, Loss: 0.0058197360982897295\n",
      "Iteration: 504, Loss: 0.005807036020203763\n",
      "Iteration: 505, Loss: 0.005794389230011604\n",
      "Iteration: 506, Loss: 0.005781795398114126\n",
      "Iteration: 507, Loss: 0.005769254197601078\n",
      "Iteration: 508, Loss: 0.0057567653042238116\n",
      "Iteration: 509, Loss: 0.0057443283963684805\n",
      "Iteration: 510, Loss: 0.005731943155029456\n",
      "Iteration: 511, Loss: 0.005719609263783113\n",
      "Iteration: 512, Loss: 0.005707326408761921\n",
      "Iteration: 513, Loss: 0.005695094278628788\n",
      "Iteration: 514, Loss: 0.0056829125645518185\n",
      "Iteration: 515, Loss: 0.005670780960179247\n",
      "Iteration: 516, Loss: 0.005658699161614794\n",
      "Iteration: 517, Loss: 0.005646666867393311\n",
      "Iteration: 518, Loss: 0.005634683778456443\n",
      "Iteration: 519, Loss: 0.005622749598129069\n",
      "Iteration: 520, Loss: 0.0056108640320955624\n",
      "Iteration: 521, Loss: 0.0055990267883765914\n",
      "Iteration: 522, Loss: 0.005587237577306076\n",
      "Iteration: 523, Loss: 0.005575496111508512\n",
      "Iteration: 524, Loss: 0.005563802105876475\n",
      "Iteration: 525, Loss: 0.005552155277548393\n",
      "Iteration: 526, Loss: 0.005540555345886674\n",
      "Iteration: 527, Loss: 0.005529002032455918\n",
      "Iteration: 528, Loss: 0.0055174950610015795\n",
      "Iteration: 529, Loss: 0.005506034157428747\n",
      "Iteration: 530, Loss: 0.005494619049781169\n",
      "Iteration: 531, Loss: 0.005483249468220571\n",
      "Iteration: 532, Loss: 0.005471925145006254\n",
      "Iteration: 533, Loss: 0.005460645814474781\n",
      "Iteration: 534, Loss: 0.005449411213020113\n",
      "Iteration: 535, Loss: 0.005438221079073624\n",
      "Iteration: 536, Loss: 0.005427075153084814\n",
      "Iteration: 537, Loss: 0.005415973177501849\n",
      "Iteration: 538, Loss: 0.005404914896752533\n",
      "Iteration: 539, Loss: 0.005393900057225369\n",
      "Iteration: 540, Loss: 0.005382928407250952\n",
      "Iteration: 541, Loss: 0.005371999697083542\n",
      "Iteration: 542, Loss: 0.005361113678882765\n",
      "Iteration: 543, Loss: 0.005350270106695573\n",
      "Iteration: 544, Loss: 0.005339468736438516\n",
      "Iteration: 545, Loss: 0.005328709325880027\n",
      "Iteration: 546, Loss: 0.005317991634622981\n",
      "Iteration: 547, Loss: 0.005307315424087532\n",
      "Iteration: 548, Loss: 0.005296680457493966\n",
      "Iteration: 549, Loss: 0.0052860864998460246\n",
      "Iteration: 550, Loss: 0.005275533317914053\n",
      "Iteration: 551, Loss: 0.00526502068021863\n",
      "Iteration: 552, Loss: 0.005254548357014234\n",
      "Iteration: 553, Loss: 0.0052441161202731856\n",
      "Iteration: 554, Loss: 0.00523372374366963\n",
      "Iteration: 555, Loss: 0.005223371002563847\n",
      "Iteration: 556, Loss: 0.005213057673986667\n",
      "Iteration: 557, Loss: 0.00520278353662398\n",
      "Iteration: 558, Loss: 0.005192548370801586\n",
      "Iteration: 559, Loss: 0.005182351958470086\n",
      "Iteration: 560, Loss: 0.005172194083189982\n",
      "Iteration: 561, Loss: 0.005162074530116911\n",
      "Iteration: 562, Loss: 0.0051519930859870375\n",
      "Iteration: 563, Loss: 0.005141949539102713\n",
      "Iteration: 564, Loss: 0.005131943679318091\n",
      "Iteration: 565, Loss: 0.005121975298025164\n",
      "Iteration: 566, Loss: 0.005112044188139618\n",
      "Iteration: 567, Loss: 0.005102150144087174\n",
      "Iteration: 568, Loss: 0.005092292961789871\n",
      "Iteration: 569, Loss: 0.005082472438652534\n",
      "Iteration: 570, Loss: 0.005072688373549447\n",
      "Iteration: 571, Loss: 0.0050629405668111225\n",
      "Iteration: 572, Loss: 0.005053228820211193\n",
      "Iteration: 573, Loss: 0.005043552936953438\n",
      "Iteration: 574, Loss: 0.005033912721659097\n",
      "Iteration: 575, Loss: 0.005024307980354105\n",
      "Iteration: 576, Loss: 0.00501473852045662\n",
      "Iteration: 577, Loss: 0.005005204150764534\n",
      "Iteration: 578, Loss: 0.00499570468144332\n",
      "Iteration: 579, Loss: 0.004986239924013827\n",
      "Iteration: 580, Loss: 0.004976809691340279\n",
      "Iteration: 581, Loss: 0.0049674137976184534\n",
      "Iteration: 582, Loss: 0.004958052058363808\n",
      "Iteration: 583, Loss: 0.004948724290399968\n",
      "Iteration: 584, Loss: 0.004939430311847147\n",
      "Iteration: 585, Loss: 0.0049301699421107955\n",
      "Iteration: 586, Loss: 0.0049209430018703\n",
      "Iteration: 587, Loss: 0.004911749313067853\n",
      "Iteration: 588, Loss: 0.004902588698897422\n",
      "Iteration: 589, Loss: 0.004893460983793797\n",
      "Iteration: 590, Loss: 0.004884365993421857\n",
      "Iteration: 591, Loss: 0.004875303554665785\n",
      "Iteration: 592, Loss: 0.004866273495618514\n",
      "Iteration: 593, Loss: 0.0048572756455713025\n",
      "Iteration: 594, Loss: 0.004848309835003372\n",
      "Iteration: 595, Loss: 0.004839375895571479\n",
      "Iteration: 596, Loss: 0.004830473660100068\n",
      "Iteration: 597, Loss: 0.004821602962570934\n",
      "Iteration: 598, Loss: 0.004812763638113409\n",
      "Iteration: 599, Loss: 0.0048039555229945265\n",
      "Iteration: 600, Loss: 0.004795178454609248\n",
      "Iteration: 601, Loss: 0.004786432271470873\n",
      "Iteration: 602, Loss: 0.004777716813201367\n",
      "Iteration: 603, Loss: 0.004769031920522094\n",
      "Iteration: 604, Loss: 0.004760377435244295\n",
      "Iteration: 605, Loss: 0.004751753200259921\n",
      "Iteration: 606, Loss: 0.004743159059532524\n",
      "Iteration: 607, Loss: 0.004734594858088028\n",
      "Iteration: 608, Loss: 0.004726060442005943\n",
      "Iteration: 609, Loss: 0.004717555658410267\n",
      "Iteration: 610, Loss: 0.0047090803554608795\n",
      "Iteration: 611, Loss: 0.004700634382344738\n",
      "Iteration: 612, Loss: 0.004692217589267295\n",
      "Iteration: 613, Loss: 0.004683829827443856\n",
      "Iteration: 614, Loss: 0.004675470949091262\n",
      "Iteration: 615, Loss: 0.004667140807419508\n",
      "Iteration: 616, Loss: 0.004658839256623358\n",
      "Iteration: 617, Loss: 0.004650566151874271\n",
      "Iteration: 618, Loss: 0.004642321349312148\n",
      "Iteration: 619, Loss: 0.004634104706037439\n",
      "Iteration: 620, Loss: 0.004625916080103059\n",
      "Iteration: 621, Loss: 0.004617755330506637\n",
      "Iteration: 622, Loss: 0.0046096223171825275\n",
      "Iteration: 623, Loss: 0.004601516900994303\n",
      "Iteration: 624, Loss: 0.004593438943726947\n",
      "Iteration: 625, Loss: 0.004585388308079322\n",
      "Iteration: 626, Loss: 0.004577364857656733\n",
      "Iteration: 627, Loss: 0.004569368456963363\n",
      "Iteration: 628, Loss: 0.004561398971395072\n",
      "Iteration: 629, Loss: 0.00455345626723202\n",
      "Iteration: 630, Loss: 0.004545540211631508\n",
      "Iteration: 631, Loss: 0.004537650672620757\n",
      "Iteration: 632, Loss: 0.004529787519089935\n",
      "Iteration: 633, Loss: 0.004521950620785097\n",
      "Iteration: 634, Loss: 0.004514139848301268\n",
      "Iteration: 635, Loss: 0.004506355073075579\n",
      "Iteration: 636, Loss: 0.004498596167380413\n",
      "Iteration: 637, Loss: 0.004490863004316769\n",
      "Iteration: 638, Loss: 0.004483155457807474\n",
      "Iteration: 639, Loss: 0.0044754734025906975\n",
      "Iteration: 640, Loss: 0.004467816714213325\n",
      "Iteration: 641, Loss: 0.004460185269024465\n",
      "Iteration: 642, Loss: 0.004452578944169056\n",
      "Iteration: 643, Loss: 0.004444997617581604\n",
      "Iteration: 644, Loss: 0.004437441167979706\n",
      "Iteration: 645, Loss: 0.004429909474857933\n",
      "Iteration: 646, Loss: 0.004422402418481618\n",
      "Iteration: 647, Loss: 0.004414919879880757\n",
      "Iteration: 648, Loss: 0.004407461740843912\n",
      "Iteration: 649, Loss: 0.004400027883912211\n",
      "Iteration: 650, Loss: 0.0043926181923734705\n",
      "Iteration: 651, Loss: 0.004385232550256197\n",
      "Iteration: 652, Loss: 0.004377870842323812\n",
      "Iteration: 653, Loss: 0.004370532954068871\n",
      "Iteration: 654, Loss: 0.004363218771707321\n",
      "Iteration: 655, Loss: 0.004355928182172888\n",
      "Iteration: 656, Loss: 0.004348661073111303\n",
      "Iteration: 657, Loss: 0.004341417332874911\n",
      "Iteration: 658, Loss: 0.0043341968505170485\n",
      "Iteration: 659, Loss: 0.004326999515786659\n",
      "Iteration: 660, Loss: 0.004319825219122707\n",
      "Iteration: 661, Loss: 0.004312673851649092\n",
      "Iteration: 662, Loss: 0.004305545305169113\n",
      "Iteration: 663, Loss: 0.004298439472160267\n",
      "Iteration: 664, Loss: 0.004291356245769052\n",
      "Iteration: 665, Loss: 0.004284295519805784\n",
      "Iteration: 666, Loss: 0.004277257188739494\n",
      "Iteration: 667, Loss: 0.004270241147692837\n",
      "Iteration: 668, Loss: 0.004263247292437087\n",
      "Iteration: 669, Loss: 0.004256275519387092\n",
      "Iteration: 670, Loss: 0.004249325725596475\n",
      "Iteration: 671, Loss: 0.0042423978087525694\n",
      "Iteration: 672, Loss: 0.004235491667171756\n",
      "Iteration: 673, Loss: 0.004228607199794506\n",
      "Iteration: 674, Loss: 0.004221744306180742\n",
      "Iteration: 675, Loss: 0.00421490288650503\n",
      "Iteration: 676, Loss: 0.0042080828415519975\n",
      "Iteration: 677, Loss: 0.004201284072711598\n",
      "Iteration: 678, Loss: 0.004194506481974693\n",
      "Iteration: 679, Loss: 0.004187749971928318\n",
      "Iteration: 680, Loss: 0.0041810144457512825\n",
      "Iteration: 681, Loss: 0.004174299807209712\n",
      "Iteration: 682, Loss: 0.004167605960652582\n",
      "Iteration: 683, Loss: 0.004160932811007351\n",
      "Iteration: 684, Loss: 0.004154280263775689\n",
      "Iteration: 685, Loss: 0.00414764822502898\n",
      "Iteration: 686, Loss: 0.004141036601404319\n",
      "Iteration: 687, Loss: 0.004134445300100027\n",
      "Iteration: 688, Loss: 0.004127874228871675\n",
      "Iteration: 689, Loss: 0.004121323296027789\n",
      "Iteration: 690, Loss: 0.004114792410425765\n",
      "Iteration: 691, Loss: 0.004108281481467867\n",
      "Iteration: 692, Loss: 0.004101790419097046\n",
      "Iteration: 693, Loss: 0.004095319133793048\n",
      "Iteration: 694, Loss: 0.0040888675365684315\n",
      "Iteration: 695, Loss: 0.004082435538964561\n",
      "Iteration: 696, Loss: 0.004076023053047743\n",
      "Iteration: 697, Loss: 0.004069629991405378\n",
      "Iteration: 698, Loss: 0.0040632562671421195\n",
      "Iteration: 699, Loss: 0.004056901793876025\n",
      "Iteration: 700, Loss: 0.004050566485734907\n",
      "Iteration: 701, Loss: 0.0040442502573524095\n",
      "Iteration: 702, Loss: 0.004037953023864497\n",
      "Iteration: 703, Loss: 0.00403167470090568\n",
      "Iteration: 704, Loss: 0.004025415204605438\n",
      "Iteration: 705, Loss: 0.0040191744515844915\n",
      "Iteration: 706, Loss: 0.004012952358951433\n",
      "Iteration: 707, Loss: 0.004006748844298997\n",
      "Iteration: 708, Loss: 0.004000563825700634\n",
      "Iteration: 709, Loss: 0.003994397221706989\n",
      "Iteration: 710, Loss: 0.00398824895134253\n",
      "Iteration: 711, Loss: 0.0039821189341020585\n",
      "Iteration: 712, Loss: 0.003976007089947258\n",
      "Iteration: 713, Loss: 0.003969913339303503\n",
      "Iteration: 714, Loss: 0.0039638376030563804\n",
      "Iteration: 715, Loss: 0.003957779802548421\n",
      "Iteration: 716, Loss: 0.003951739859575814\n",
      "Iteration: 717, Loss: 0.0039457176963852356\n",
      "Iteration: 718, Loss: 0.0039397132356704875\n",
      "Iteration: 719, Loss: 0.003933726400569448\n",
      "Iteration: 720, Loss: 0.0039277571146608195\n",
      "Iteration: 721, Loss: 0.0039218053019609775\n",
      "Iteration: 722, Loss: 0.003915870886920885\n",
      "Iteration: 723, Loss: 0.003909953794423069\n",
      "Iteration: 724, Loss: 0.003904053949778461\n",
      "Iteration: 725, Loss: 0.0038981712787234026\n",
      "Iteration: 726, Loss: 0.0038923057074166257\n",
      "Iteration: 727, Loss: 0.003886457162436286\n",
      "Iteration: 728, Loss: 0.003880625570777033\n",
      "Iteration: 729, Loss: 0.003874810859846989\n",
      "Iteration: 730, Loss: 0.003869012957464912\n",
      "Iteration: 731, Loss: 0.003863231791857291\n",
      "Iteration: 732, Loss: 0.0038574672916554895\n",
      "Iteration: 733, Loss: 0.0038517193858929504\n",
      "Iteration: 734, Loss: 0.0038459880040022203\n",
      "Iteration: 735, Loss: 0.003840273075812427\n",
      "Iteration: 736, Loss: 0.0038345745315462552\n",
      "Iteration: 737, Loss: 0.0038288923018173623\n",
      "Iteration: 738, Loss: 0.003823226317627645\n",
      "Iteration: 739, Loss: 0.0038175765103644565\n",
      "Iteration: 740, Loss: 0.003811942811797967\n",
      "Iteration: 741, Loss: 0.0038063251540786406\n",
      "Iteration: 742, Loss: 0.003800723469734331\n",
      "Iteration: 743, Loss: 0.003795137691667965\n",
      "Iteration: 744, Loss: 0.0037895677531547736\n",
      "Iteration: 745, Loss: 0.003784013587839769\n",
      "Iteration: 746, Loss: 0.0037784751297352394\n",
      "Iteration: 747, Loss: 0.0037729523132181645\n",
      "Iteration: 748, Loss: 0.0037674450730277222\n",
      "Iteration: 749, Loss: 0.0037619533442628753\n",
      "Iteration: 750, Loss: 0.003756477062379841\n",
      "Iteration: 751, Loss: 0.003751016163189619\n",
      "Iteration: 752, Loss: 0.0037455705828556866\n",
      "Iteration: 753, Loss: 0.003740140257891454\n",
      "Iteration: 754, Loss: 0.003734725125158017\n",
      "Iteration: 755, Loss: 0.0037293251218616366\n",
      "Iteration: 756, Loss: 0.0037239401855516064\n",
      "Iteration: 757, Loss: 0.0037185702541177312\n",
      "Iteration: 758, Loss: 0.0037132152657880567\n",
      "Iteration: 759, Loss: 0.0037078751591267333\n",
      "Iteration: 760, Loss: 0.0037025498730315\n",
      "Iteration: 761, Loss: 0.003697239346731671\n",
      "Iteration: 762, Loss: 0.0036919435197857588\n",
      "Iteration: 763, Loss: 0.003686662332079282\n",
      "Iteration: 764, Loss: 0.0036813957238226013\n",
      "Iteration: 765, Loss: 0.003676143635548704\n",
      "Iteration: 766, Loss: 0.0036709060081110707\n",
      "Iteration: 767, Loss: 0.003665682782681536\n",
      "Iteration: 768, Loss: 0.0036604739007481167\n",
      "Iteration: 769, Loss: 0.0036552793041129223\n",
      "Iteration: 770, Loss: 0.003650098934890077\n",
      "Iteration: 771, Loss: 0.003644932735503664\n",
      "Iteration: 772, Loss: 0.0036397806486855406\n",
      "Iteration: 773, Loss: 0.0036346426174734946\n",
      "Iteration: 774, Loss: 0.003629518585209053\n",
      "Iteration: 775, Loss: 0.0036244084955354755\n",
      "Iteration: 776, Loss: 0.0036193122923958983\n",
      "Iteration: 777, Loss: 0.0036142299200311824\n",
      "Iteration: 778, Loss: 0.003609161322978087\n",
      "Iteration: 779, Loss: 0.0036041064460672137\n",
      "Iteration: 780, Loss: 0.003599065234421152\n",
      "Iteration: 781, Loss: 0.0035940376334524793\n",
      "Iteration: 782, Loss: 0.0035890235888619594\n",
      "Iteration: 783, Loss: 0.00358402304663657\n",
      "Iteration: 784, Loss: 0.003579035953047649\n",
      "Iteration: 785, Loss: 0.0035740622546490816\n",
      "Iteration: 786, Loss: 0.0035691018982753333\n",
      "Iteration: 787, Loss: 0.003564154831039821\n",
      "Iteration: 788, Loss: 0.0035592210003328848\n",
      "Iteration: 789, Loss: 0.003554300353820098\n",
      "Iteration: 790, Loss: 0.0035493928394404843\n",
      "Iteration: 791, Loss: 0.0035444984054047134\n",
      "Iteration: 792, Loss: 0.003539617000193321\n",
      "Iteration: 793, Loss: 0.0035347485725550102\n",
      "Iteration: 794, Loss: 0.0035298930715049093\n",
      "Iteration: 795, Loss: 0.003525050446322781\n",
      "Iteration: 796, Loss: 0.003520220646551415\n",
      "Iteration: 797, Loss: 0.003515403621994895\n",
      "Iteration: 798, Loss: 0.00351059932271689\n",
      "Iteration: 799, Loss: 0.0035058076990390243\n",
      "Iteration: 800, Loss: 0.003501028701539223\n",
      "Iteration: 801, Loss: 0.0034962622810500036\n",
      "Iteration: 802, Loss: 0.0034915083886569416\n",
      "Iteration: 803, Loss: 0.0034867669756970116\n",
      "Iteration: 804, Loss: 0.003482037993756968\n",
      "Iteration: 805, Loss: 0.0034773213946717586\n",
      "Iteration: 806, Loss: 0.0034726171305229587\n",
      "Iteration: 807, Loss: 0.00346792515363718\n",
      "Iteration: 808, Loss: 0.003463245416584569\n",
      "Iteration: 809, Loss: 0.0034585778721771673\n",
      "Iteration: 810, Loss: 0.0034539224734674626\n",
      "Iteration: 811, Loss: 0.0034492791737468173\n",
      "Iteration: 812, Loss: 0.0034446479265440354\n",
      "Iteration: 813, Loss: 0.0034400286856237296\n",
      "Iteration: 814, Loss: 0.00343542140498492\n",
      "Iteration: 815, Loss: 0.003430826038859613\n",
      "Iteration: 816, Loss: 0.003426242541711237\n",
      "Iteration: 817, Loss: 0.0034216708682331754\n",
      "Iteration: 818, Loss: 0.0034171109733474436\n",
      "Iteration: 819, Loss: 0.0034125628122031767\n",
      "Iteration: 820, Loss: 0.003408026340175155\n",
      "Iteration: 821, Loss: 0.0034035015128624927\n",
      "Iteration: 822, Loss: 0.003398988286087223\n",
      "Iteration: 823, Loss: 0.0033944866158928688\n",
      "Iteration: 824, Loss: 0.0033899964585430422\n",
      "Iteration: 825, Loss: 0.0033855177705201323\n",
      "Iteration: 826, Loss: 0.0033810505085239825\n",
      "Iteration: 827, Loss: 0.003376594629470374\n",
      "Iteration: 828, Loss: 0.0033721500904898617\n",
      "Iteration: 829, Loss: 0.003367716848926403\n",
      "Iteration: 830, Loss: 0.003363294862335971\n",
      "Iteration: 831, Loss: 0.0033588840884853184\n",
      "Iteration: 832, Loss: 0.0033544844853505822\n",
      "Iteration: 833, Loss: 0.0033500960111161474\n",
      "Iteration: 834, Loss: 0.0033457186241732267\n",
      "Iteration: 835, Loss: 0.0033413522831186706\n",
      "Iteration: 836, Loss: 0.003336996946753626\n",
      "Iteration: 837, Loss: 0.0033326525740823795\n",
      "Iteration: 838, Loss: 0.003328319124311035\n",
      "Iteration: 839, Loss: 0.003323996556846365\n",
      "Iteration: 840, Loss: 0.003319684831294451\n",
      "Iteration: 841, Loss: 0.003315383907459578\n",
      "Iteration: 842, Loss: 0.003311093745343029\n",
      "Iteration: 843, Loss: 0.003306814305141823\n",
      "Iteration: 844, Loss: 0.00330254554724753\n",
      "Iteration: 845, Loss: 0.0032982874322451594\n",
      "Iteration: 846, Loss: 0.0032940399209119334\n",
      "Iteration: 847, Loss: 0.003289802974216065\n",
      "Iteration: 848, Loss: 0.0032855765533157553\n",
      "Iteration: 849, Loss: 0.0032813606195578977\n",
      "Iteration: 850, Loss: 0.0032771551344770444\n",
      "Iteration: 851, Loss: 0.003272960059794161\n",
      "Iteration: 852, Loss: 0.0032687753574156364\n",
      "Iteration: 853, Loss: 0.0032646009894320765\n",
      "Iteration: 854, Loss: 0.0032604369181172257\n",
      "Iteration: 855, Loss: 0.0032562831059268302\n",
      "Iteration: 856, Loss: 0.003252139515497619\n",
      "Iteration: 857, Loss: 0.003248006109646203\n",
      "Iteration: 858, Loss: 0.003243882851367909\n",
      "Iteration: 859, Loss: 0.0032397697038358455\n",
      "Iteration: 860, Loss: 0.0032356666303997514\n",
      "Iteration: 861, Loss: 0.003231573594584971\n",
      "Iteration: 862, Loss: 0.003227490560091395\n",
      "Iteration: 863, Loss: 0.003223417490792493\n",
      "Iteration: 864, Loss: 0.0032193543507341403\n",
      "Iteration: 865, Loss: 0.0032153011041337423\n",
      "Iteration: 866, Loss: 0.003211257715379117\n",
      "Iteration: 867, Loss: 0.003207224149027592\n",
      "Iteration: 868, Loss: 0.0032032003698048176\n",
      "Iteration: 869, Loss: 0.0031991863426040342\n",
      "Iteration: 870, Loss: 0.0031951820324848043\n",
      "Iteration: 871, Loss: 0.003191187404672251\n",
      "Iteration: 872, Loss: 0.0031872024245559907\n",
      "Iteration: 873, Loss: 0.0031832270576891766\n",
      "Iteration: 874, Loss: 0.003179261269787537\n",
      "Iteration: 875, Loss: 0.0031753050267284135\n",
      "Iteration: 876, Loss: 0.0031713582945498125\n",
      "Iteration: 877, Loss: 0.00316742103944953\n",
      "Iteration: 878, Loss: 0.0031634932277841343\n",
      "Iteration: 879, Loss: 0.003159574826068041\n",
      "Iteration: 880, Loss: 0.0031556658009726625\n",
      "Iteration: 881, Loss: 0.003151766119325477\n",
      "Iteration: 882, Loss: 0.0031478757481090145\n",
      "Iteration: 883, Loss: 0.003143994654460116\n",
      "Iteration: 884, Loss: 0.003140122805668941\n",
      "Iteration: 885, Loss: 0.003136260169178101\n",
      "Iteration: 886, Loss: 0.003132406712581722\n",
      "Iteration: 887, Loss: 0.003128562403624696\n",
      "Iteration: 888, Loss: 0.003124727210201703\n",
      "Iteration: 889, Loss: 0.0031209011003563675\n",
      "Iteration: 890, Loss: 0.003117084042280412\n",
      "Iteration: 891, Loss: 0.003113276004312794\n",
      "Iteration: 892, Loss: 0.0031094769549389013\n",
      "Iteration: 893, Loss: 0.0031056868627896816\n",
      "Iteration: 894, Loss: 0.003101905696640737\n",
      "Iteration: 895, Loss: 0.0030981334254116134\n",
      "Iteration: 896, Loss: 0.003094370018164923\n",
      "Iteration: 897, Loss: 0.0030906154441055047\n",
      "Iteration: 898, Loss: 0.0030868696725796386\n",
      "Iteration: 899, Loss: 0.0030831326730742523\n",
      "Iteration: 900, Loss: 0.0030794044152160746\n",
      "Iteration: 901, Loss: 0.003075684868770868\n",
      "Iteration: 902, Loss: 0.0030719740036426572\n",
      "Iteration: 903, Loss: 0.0030682717898728954\n",
      "Iteration: 904, Loss: 0.0030645781976397137\n",
      "Iteration: 905, Loss: 0.0030608931972571926\n",
      "Iteration: 906, Loss: 0.00305721675917445\n",
      "Iteration: 907, Loss: 0.003053548853975056\n",
      "Iteration: 908, Loss: 0.003049889452376135\n",
      "Iteration: 909, Loss: 0.003046238525227671\n",
      "Iteration: 910, Loss: 0.0030425960435117843\n",
      "Iteration: 911, Loss: 0.0030389619783419276\n",
      "Iteration: 912, Loss: 0.0030353363009621758\n",
      "Iteration: 913, Loss: 0.0030317189827465075\n",
      "Iteration: 914, Loss: 0.003028109995198018\n",
      "Iteration: 915, Loss: 0.0030245093099482865\n",
      "Iteration: 916, Loss: 0.003020916898756593\n",
      "Iteration: 917, Loss: 0.0030173327335092006\n",
      "Iteration: 918, Loss: 0.0030137567862186334\n",
      "Iteration: 919, Loss: 0.0030101890290231193\n",
      "Iteration: 920, Loss: 0.0030066294341856313\n",
      "Iteration: 921, Loss: 0.0030030779740934147\n",
      "Iteration: 922, Loss: 0.002999534621257202\n",
      "Iteration: 923, Loss: 0.0029959993483105055\n",
      "Iteration: 924, Loss: 0.002992472128009009\n",
      "Iteration: 925, Loss: 0.002988952933229817\n",
      "Iteration: 926, Loss: 0.0029854417369708456\n",
      "Iteration: 927, Loss: 0.0029819385123501033\n",
      "Iteration: 928, Loss: 0.0029784432326050462\n",
      "Iteration: 929, Loss: 0.0029749558710919256\n",
      "Iteration: 930, Loss: 0.0029714764012851074\n",
      "Iteration: 931, Loss: 0.0029680047967764866\n",
      "Iteration: 932, Loss: 0.0029645410312747434\n",
      "Iteration: 933, Loss: 0.0029610850786047933\n",
      "Iteration: 934, Loss: 0.0029576369127070623\n",
      "Iteration: 935, Loss: 0.0029541965076369517\n",
      "Iteration: 936, Loss: 0.0029507638375640935\n",
      "Iteration: 937, Loss: 0.002947338876771846\n",
      "Iteration: 938, Loss: 0.002943921599656568\n",
      "Iteration: 939, Loss: 0.0029405119807270706\n",
      "Iteration: 940, Loss: 0.0029371099946039723\n",
      "Iteration: 941, Loss: 0.002933715616019107\n",
      "Iteration: 942, Loss: 0.0029303288198148595\n",
      "Iteration: 943, Loss: 0.002926949580943663\n",
      "Iteration: 944, Loss: 0.002923577874467332\n",
      "Iteration: 945, Loss: 0.0029202136755565027\n",
      "Iteration: 946, Loss: 0.002916856959489988\n",
      "Iteration: 947, Loss: 0.0029135077016542903\n",
      "Iteration: 948, Loss: 0.0029101658775428987\n",
      "Iteration: 949, Loss: 0.002906831462755806\n",
      "Iteration: 950, Loss: 0.002903504432998887\n",
      "Iteration: 951, Loss: 0.0029001847640833427\n",
      "Iteration: 952, Loss: 0.002896872431925118\n",
      "Iteration: 953, Loss: 0.0028935674125443794\n",
      "Iteration: 954, Loss: 0.002890269682064896\n",
      "Iteration: 955, Loss: 0.002886979216713533\n",
      "Iteration: 956, Loss: 0.002883695992819638\n",
      "Iteration: 957, Loss: 0.002880419986814595\n",
      "Iteration: 958, Loss: 0.0028771511752311617\n",
      "Iteration: 959, Loss: 0.0028738895347030008\n",
      "Iteration: 960, Loss: 0.002870635041964111\n",
      "Iteration: 961, Loss: 0.002867387673848318\n",
      "Iteration: 962, Loss: 0.0028641474072887132\n",
      "Iteration: 963, Loss: 0.0028609142193171165\n",
      "Iteration: 964, Loss: 0.0028576880870636295\n",
      "Iteration: 965, Loss: 0.0028544689877559653\n",
      "Iteration: 966, Loss: 0.002851256898719118\n",
      "Iteration: 967, Loss: 0.0028480517973746974\n",
      "Iteration: 968, Loss: 0.0028448536612404596\n",
      "Iteration: 969, Loss: 0.002841662467929814\n",
      "Iteration: 970, Loss: 0.0028384781951513397\n",
      "Iteration: 971, Loss: 0.0028353008207082286\n",
      "Iteration: 972, Loss: 0.002832130322497803\n",
      "Iteration: 973, Loss: 0.0028289666785110646\n",
      "Iteration: 974, Loss: 0.0028258098668321125\n",
      "Iteration: 975, Loss: 0.0028226598656377463\n",
      "Iteration: 976, Loss: 0.002819516653196918\n",
      "Iteration: 977, Loss: 0.0028163802078702846\n",
      "Iteration: 978, Loss: 0.0028132505081096973\n",
      "Iteration: 979, Loss: 0.002810127532457729\n",
      "Iteration: 980, Loss: 0.002807011259547234\n",
      "Iteration: 981, Loss: 0.002803901668100862\n",
      "Iteration: 982, Loss: 0.002800798736930501\n",
      "Iteration: 983, Loss: 0.002797702444936979\n",
      "Iteration: 984, Loss: 0.002794612771109454\n",
      "Iteration: 985, Loss: 0.0027915296945250434\n",
      "Iteration: 986, Loss: 0.0027884531943482833\n",
      "Iteration: 987, Loss: 0.0027853832498307795\n",
      "Iteration: 988, Loss: 0.0027823198403106805\n",
      "Iteration: 989, Loss: 0.002779262945212249\n",
      "Iteration: 990, Loss: 0.0027762125440453766\n",
      "Iteration: 991, Loss: 0.0027731686164052397\n",
      "Iteration: 992, Loss: 0.00277013114197178\n",
      "Iteration: 993, Loss: 0.002767100100509262\n",
      "Iteration: 994, Loss: 0.002764075471865882\n",
      "Iteration: 995, Loss: 0.002761057235973306\n",
      "Iteration: 996, Loss: 0.002758045372846242\n",
      "Iteration: 997, Loss: 0.002755039862582049\n",
      "Iteration: 998, Loss: 0.0027520406853602346\n",
      "Iteration: 999, Loss: 0.002749047821442128\n"
     ]
    }
   ],
   "source": [
    "step = 0.01\n",
    "trainingIterations = 1000\n",
    "    \n",
    "for iteration in range(trainingIterations):\n",
    "    # forward pass\n",
    "    predictedOutputs = [mlp(inputs) for inputs in inputDataSet]\n",
    "\n",
    "    # calculate loss with mean squared error\n",
    "    loss = sum((predicted - desired)**2 for desired, predicted in zip(desiredOutputs, predictedOutputs))\n",
    "\n",
    "    # zero all gradients\n",
    "    for param in mlp.parameters():\n",
    "        param.gradient = 0.0\n",
    "        \n",
    "    # backpropagation calculates all gradients in network\n",
    "    loss.backward()\n",
    "\n",
    "    # stochastic gradient descent - adjust weights and biases in network to minimize loss function\n",
    "    for param in mlp.parameters():\n",
    "        param.data += param.gradient * -step # minus because we want to go in opposite direction of gradient\n",
    "        \n",
    "    print(f'Iteration: {iteration}, Loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9826680041072992),\n",
       " Value(data=-0.9806385134889171),\n",
       " Value(data=-0.9704972316154413),\n",
       " Value(data=0.9653103871808034)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted outputs after training\n",
    "predictedOutputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
