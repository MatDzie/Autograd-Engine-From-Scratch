# **Autograd engine from scratch**

Tiny scalar Autograd engine that implements backpropagation over dynamically build Directed Acyclic Graph and a small neural network library on top of it with a PyTorch-like API.

This is the code I created as part of learning neural networks during Andrej Karpathy lecture series "Neural Networks: Zero to Hero".

## Key topics learned during implementation:
- derivative/gradient
- chain rule
- neural network definition
- weights
- bias
- activation function
- tensor
- forward pass
- backpropagation
- neuron
- layer
- multi-layer perceptron
- loss function

## Prerequisites

- `pip install ipynb`
- `pip install import-ipynb`
- `pip install graphviz`
- `sudo apt install graphviz`

## License
MIT